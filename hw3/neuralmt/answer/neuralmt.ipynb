{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuralmt program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2Seq' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# loading test dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_iter \u001b[38;5;241m=\u001b[39m loadTestData(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m                             device\u001b[38;5;241m=\u001b[39mdevice, linesToLoad\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mmaxsize)\n\u001b[1;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m translate(model, test_iter) \u001b[38;5;66;03m# Warning: will take >5mins depending on your machine\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(results))\n",
      "File \u001b[0;32m~/Desktop/t4/NLP/A/nlpclass-1241-g-teamthinkers/hw3/neuralmt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2Seq' object has no attribute 'fields'"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bleu_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbleu_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bleu\n\u001b[1;32m      2\u001b[0m ref_t \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev.out\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m r:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bleu_check'"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This program uses 2 Gated Recurrent Units (GRU) in a sequence to sequence model with an additional attention mdoel to translate German into English using python 3.8+.\n",
    "\n",
    "### Components\n",
    "\n",
    "### 1. AttentionModule\n",
    "calculates the attention weights \n",
    "\n",
    "#### calcAlpha(self, decoder_hidden, encoder_out):\n",
    "Computes the attention score using the encoders output and the decoders current hidden state. The attention score is calculated by applying the hyperbolic tangent fucntion (tanh) on the encoders output and decoders hidden state, performs a matrix transformation then applys the softmax function to calculate the alpha.\n",
    "#### forward(self, decoder_hidden, encoder_out):\n",
    "Computes and returns the context vector and alpha using the calcAlpha function.\n",
    "The context vector is calculated by multipying alpha from calcAlpha with the encoders hidden state and summing up over all index i.\n",
    "\n",
    "### 2. def greedyDecoder(decoder, encoder_out, encoder_hidden, maxLen)\n",
    "Translates the output using a greedy algorithm to select the highest probabilty token and implements unknown token replacement.\n",
    "\n",
    "### 3. def translate(model, input_dl):\n",
    "Uses the model to translate the input sequence to the desired output sequence. In this program it converts a German sequence input into an English output.\n",
    "\n",
    "### 4. hp\n",
    "Defines all the hyper parameters used in the encoder, decoder and the dataset classes \n",
    "\n",
    "Parameters:\n",
    "\n",
    "-   pad_idx: padding index \n",
    "-   sos_idx: start of sequence index\n",
    "-  eos_idx: end of sequence index\n",
    "-  unk_idx: unknown token index\n",
    "-  lex_min_freq = minimum frequency for the lexicon\n",
    "\n",
    "\n",
    "-   hidden_dim = hidden dimension\n",
    "-   embed_dim = embedding dimension\n",
    "-   n_layers = layers\n",
    "-   dropout = dropout rate\n",
    "-   batch_size = batch size\n",
    "-   num_epochs = epoch num\n",
    "-   lexicon_cap = lexicon capacity\n",
    "\n",
    "-   cycle_length: length of the cycle\n",
    "\n",
    "-   max_len: max length for the sequence to sequence model\n",
    "-   device: cude or CPU\n",
    "\n",
    "### 5. Encoder\n",
    "Uses a bidiretional GRU to contextually encode an input sequence. \n",
    "\n",
    "### 6. Decoder\n",
    "Uses a single directional GRU and the attention module to decode the output from the the encoder\n",
    "\n",
    "### 7. Seq2Seq\n",
    "Combines the Encoder and Decoder to create a single sequence to sequence model.\n",
    "\n",
    "### 8. Dataset\n",
    "A utility class that helps to manage the source data, target data and the tokenizers for German and English\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Attention Module:\n",
    "\n",
    "The program in its initial state started with a BLEU score of 1.8637.\n",
    "\n",
    "The calcAlpha function first calculates the score using the encoders output and the decoder current hidden state. Then applys the tanh function on this score. This is finally fed into a softmax function to calculate the alpha.\n",
    "\n",
    "We initially faced dimension issues with running calcAlpha. We were able to fix these issues by squeezing the scores before the softmax function. \n",
    "\n",
    "The forward function calculates the context vector using the sum of the output of calcAlpha multiplied by the decoders hidden state.\n",
    "\n",
    "Running on a subset of dev we were able to get a BLEU score of 12.0102\n",
    "Running on the full test set  we were able to get a BLEU score of 14.2427\n",
    "\n",
    "\n",
    "\n",
    "### Translate Function:\n",
    "\n",
    "Our translate function attempts to implements the unknown word replacement but was unable to find unk words in the decoders output.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Implementing an attention module greatly increases the performance of a NMT model.\n",
    "- The attention module does not require as much work as expected to create a much better translation\n",
    "\n",
    "## Conclusion:\n",
    "- What we learned\n",
    "- - For NMT using a sequence to sequence model, it is necessary to include an attention module to greatly increse the quality and accuracy of translations.\n",
    "- Future Works \n",
    "- - Fixing our unknown word replacement to see how much of an improvement it adds to the translation\n",
    "- - Implementing beam search to compared the performance gains to just unk replacement and combined with unk replacement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
