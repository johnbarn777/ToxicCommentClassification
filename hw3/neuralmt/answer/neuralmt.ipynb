{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuralmt program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralmt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[1;32md:\\CMPT413\\nlpclass-1241-g-teamthinkers\\hw3\\neuralmt\\answer\\neuralmt.py:260\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Load Tokeniser\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m token_en \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load the English model to tokenize English text\u001b[39;00m\n\u001b[0;32m    261\u001b[0m token_de \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde_core_news_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Load the German model to tokenize German text\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenise_en\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\yohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from neuralmt import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Seq2Seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2Seq\u001b[49m(build\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2seq_E049.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Seq2Seq' is not defined"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.params['srcLex'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bleu_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbleu_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bleu\n\u001b[0;32m      2\u001b[0m ref_t \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev.out\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m r:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bleu_check'"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This program uses 2 Gated Recurrent Units (GRU) in a sequence to sequence model with an additional attention mdoel to translate German into English using python 3.8+.\n",
    "\n",
    "### Components\n",
    "\n",
    "### 1. AttentionModule\n",
    "calculates the attention weights \n",
    "\n",
    "#### calcAlpha(self, decoder_hidden, encoder_out):\n",
    "Computes the attention score using the encoders output and the decoders current hidden state. The attention score is calculated by applying the hyperbolic tangent fucntion (tanh) on the encoders output and decoders hidden state, performs a matrix transformation then applys the softmax function to calculate the alpha.\n",
    "#### forward(self, decoder_hidden, encoder_out):\n",
    "Computes and returns the context vector and alpha using the calcAlpha function.\n",
    "The context vector is calculated by multipying alpha from calcAlpha with the encoders hidden state and summing up over all index i.\n",
    "\n",
    "### 2. def greedyDecoder(decoder, encoder_out, encoder_hidden, maxLen)\n",
    "Translates the output using a greedy algorithm to select the highest probabilty token and implements unknown token replacement.\n",
    "\n",
    "### 3. def translate(model, input_dl):\n",
    "Uses the model to translate the input sequence to the desired output sequence. In this program it converts a German sequence input into an English output.\n",
    "\n",
    "### 4. hp\n",
    "Defines all the hyper parameters used in the encoder, decoder and the dataset classes \n",
    "\n",
    "Parameters:\n",
    "\n",
    "-   pad_idx: padding index \n",
    "-   sos_idx: start of sequence index\n",
    "-  eos_idx: end of sequence index\n",
    "-  unk_idx: unknown token index\n",
    "-  lex_min_freq = minimum frequency for the lexicon\n",
    "\n",
    "\n",
    "-   hidden_dim = hidden dimension\n",
    "-   embed_dim = embedding dimension\n",
    "-   n_layers = layers\n",
    "-   dropout = dropout rate\n",
    "-   batch_size = batch size\n",
    "-   num_epochs = epoch num\n",
    "-   lexicon_cap = lexicon capacity\n",
    "\n",
    "-   cycle_length: length of the cycle\n",
    "\n",
    "-   max_len: max length for the sequence to sequence model\n",
    "-   device: cude or CPU\n",
    "\n",
    "### 5. Encoder\n",
    "Uses a bidiretional GRU to contextually encode an input sequence. \n",
    "\n",
    "### 6. Decoder\n",
    "Uses a single directional GRU and the attention module to decode the output from the the encoder\n",
    "\n",
    "### 7. Seq2Seq\n",
    "Combines the Encoder and Decoder to create a single sequence to sequence model.\n",
    "\n",
    "### 8. Dataset\n",
    "A utility class that helps to manage the source data, target data and the tokenizers for German and English\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Attention Module:\n",
    "\n",
    "The program in its initial state started with a BLEU score of 1.8637.\n",
    "\n",
    "The calcAlpha function first calculates the score using the encoders output and the decoder current hidden state. Then applys the tanh function on this score. This is finally fed into a softmax function to calculate the alpha.\n",
    "\n",
    "We initially faced dimension issues with running calcAlpha. We were able to fix these issues by squeezing the scores before the softmax function. \n",
    "\n",
    "The forward function calculates the context vector using the sum of the output of calcAlpha multiplied by the decoders hidden state.\n",
    "\n",
    "Running on a subset of dev we were able to get a BLEU score of 12.0102\n",
    "Running on the full test set  we were able to get a BLEU score of 14.2427\n",
    "\n",
    "\n",
    "\n",
    "### Translate Function:\n",
    "\n",
    "Our translate function attempts to implements the unknown word replacement but was unable to find unk words in the decoders output.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Implementing an attention module greatly increases the performance of a NMT model.\n",
    "- The attention module does not require as much work as expected to create a much better translation\n",
    "\n",
    "## Conclusion:\n",
    "- What we learned\n",
    "- - For NMT using a sequence to sequence model, it is necessary to include an attention module to greatly increse the quality and accuracy of translations.\n",
    "- Future Works \n",
    "- - Fixing our unknown word replacement to see how much of an improvement it adds to the translation\n",
    "- - Implementing beam search to compared the performance gains to just unk replacement and combined with unk replacement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
