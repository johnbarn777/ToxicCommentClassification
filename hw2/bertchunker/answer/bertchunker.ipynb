{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bertchunker Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Solution on Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "b:\\nlpclass-1241-g-teamthinkers\\hw2\\bertchunker\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kyleb\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 483/483 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.63MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 12.6MB/s]\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:03<00:00, 76.4MB/s] \n",
      "100%|██████████| 1027/1027 [00:34<00:00, 30.18it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = FinetuneTagger(os.path.join('..', 'data', 'chunker'), modelsuffix='.pt')\n",
    "decoder_output = chunker.decode(os.path.join('..', 'data', 'input', 'dev.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 12023 phrases; correct: 11264.\n",
      "accuracy:  96.34%; (non-O)\n",
      "accuracy:  96.42%; precision:  93.69%; recall:  94.69%; FB1:  94.18\n",
      "             ADJP: precision:  73.03%; recall:  77.88%; FB1:  75.37  241\n",
      "             ADVP: precision:  79.70%; recall:  78.89%; FB1:  79.29  394\n",
      "            CONJP: precision:  45.45%; recall:  71.43%; FB1:  55.56  11\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  94.34%; recall:  95.17%; FB1:  94.76  6292\n",
      "               PP: precision:  97.66%; recall:  97.34%; FB1:  97.50  2433\n",
      "              PRT: precision:  69.23%; recall:  80.00%; FB1:  74.23  52\n",
      "             SBAR: precision:  90.00%; recall:  91.14%; FB1:  90.57  240\n",
      "               VP: precision:  93.43%; recall:  95.70%; FB1:  94.55  2360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93.68709972552608, 94.68728984532616, 94.18453948743677)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "sys.path.append('..')\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('..', 'data', 'reference', 'dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "## Transformer-based Sequence Tagger\n",
    "\n",
    "### Overview\n",
    "This program implements a Transformer-based model for sequence tagging, focusing on tasks like Named Entity Recognition (NER). It utilizes the Hugging Face Transformers library for model architecture and pre-trained embeddings.\n",
    "\n",
    "### Components\n",
    "\n",
    "#### 1. create_mispelling(word)\n",
    "Description: Generates misspelled versions of input words to augment training data.\n",
    "Parameters:\n",
    "- word: Input word for misspelling.\n",
    "\n",
    "Returns:\n",
    "- Misspelled version of the input word.\n",
    "\n",
    "#### 2. read_conll(handle, input_idx=0, label_idx=2)\n",
    "Description: Reads CoNLL-formatted data from a file, handling both input words and their corresponding labels.\n",
    "\n",
    "Parameters:\n",
    "- handle: File handle for reading data.\n",
    "- input_idx: Index of the input word in each line (default: 0).\n",
    "- label_idx: Index of the label in each line (default: 2).\n",
    "\n",
    "Returns:\n",
    "- List of tuples containing input words and their labels.\n",
    "\n",
    "#### 3. TransformerModel\n",
    "Description: Custom Transformer-based model for sequence tagging.\n",
    "\n",
    "Architecture:\n",
    "- Uses a pre-trained Transformer-based encoder (e.g., BERT) for contextual word representations.\n",
    "- Adds a linear layer for classification into different entity types.\n",
    "\n",
    "Methods:\n",
    "- init_model_from_scratch: Initializes the model with specified parameters.\n",
    "- forward: Defines the forward pass of the model.\n",
    "\n",
    "#### 4. FinetuneTagger\n",
    "Description: Class for training and decoding the sequence tagger model.\n",
    "\n",
    "Attributes:\n",
    "- tokenizer: Tokenizer for processing input sequences.\n",
    "- trainfile: Path to the training data file.\n",
    "- modelfile: File to save the trained model.\n",
    "- modelsuffix: Suffix for the model file.\n",
    "- basemodel: Pre-trained base model for the encoder.\n",
    "- epochs: Number of training epochs.\n",
    "- batchsize: Batch size for training.\n",
    "- lr: Learning rate for fine-tuning.\n",
    "- training_data: List of tuples containing training data.\n",
    "- tag_to_ix: Dictionary mapping tags to indices.\n",
    "- ix_to_tag: List mapping indices to tags.\n",
    "- model: Instance of the Transformer model.\n",
    "\n",
    "Methods:\n",
    "- load_training_data: Loads and preprocesses training data.\n",
    "- prepare_sequence: Prepares input sequences for training or inference.\n",
    "- argmax: Performs decoding to obtain predicted labels.\n",
    "- train: Trains the sequence tagger model.\n",
    "- model_str: Returns a string representation of the trained model.\n",
    "- decode: Decodes input sequences using the trained model.\n",
    "\n",
    "#### Usage\n",
    "1. Training:\n",
    "    - Initialize FinetuneTagger with appropriate parameters.\n",
    "    - Call the train method to train the model.\n",
    "2. Decoding:\n",
    "    - Initialize FinetuneTagger with the trained model file.\n",
    "    - Call the decode method with input data to obtain predictions.\n",
    "\n",
    "#### Dependencies\n",
    "- Python 3.x\n",
    "- PyTorch\n",
    "- Hugging Face Transformers\n",
    "- tqdm (for progress bars)\n",
    "\n",
    "#### References\n",
    "- Hugging Face Transformers Documentation: https://huggingface.co/transformers/\n",
    "- PyTorch Documentation: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Initial Approach:\n",
    "- Misspelling Function: Implemented a function to generate misspelled words, primarily swapping two characters.\n",
    "- Augmenting Training Set: Initially replaced original words in the training set with misspelled versions.\n",
    "\n",
    "### Experimentation:\n",
    "1. Increasing Variations:\n",
    "    - Expanded misspelling variations (e.g., 1 character deletion, addition, and replacement).\n",
    "    - Limited impact on model performance observed.\n",
    "\n",
    "2. Augmenting Strategy:\n",
    "    - Augmented the training set by adding misspelled versions alongside original words.\n",
    "    - Preserved original data while enhancing diversity.\n",
    "\n",
    "3. Optimizing Misspelling Rate:\n",
    "    - Found that increasing misspelling rate to 40% yielded optimal results.\n",
    "    - Balanced diversity and data integrity effectively.\n",
    "4. Doubling Training Set:\n",
    "    - Doubled training set by adding misspelled versions at an 80% rate.\n",
    "    - Substantial performance improvement achieved (~94.5% accuracy).\n",
    "\n",
    "### Observations:\n",
    "1. Effective Strategies:\n",
    "    - Augmenting training data with diverse misspellings enhanced model performance.\n",
    "    - Optimizing misspelling rate and maintaining data integrity were key factors.\n",
    "2. Trade-offs:\n",
    "    - Doubling training set improved performance but increased training time significantly.\n",
    "    - Balancing augmentation benefits with computational cost is critical.\n",
    "\n",
    "### Conclusion:\n",
    "- Effective Approaches:\n",
    "    - Diverse misspelling augmentation and optimized misspelling rate improved model accuracy.\n",
    "- Future Considerations:\n",
    "    - Further experimentation with misspelling variations and rates could refine model performance.\n",
    "    - Exploring methods to mitigate computational overhead while augmenting training data is essential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
